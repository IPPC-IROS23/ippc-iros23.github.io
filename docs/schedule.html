<!DOCTYPE html>
<html lang="en"> 
<head>
    <title>IROS 2023 IPPC Workshop Schedule</title>
	
	<!-- Meta -->
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="description" content="">
    <meta name="keywords" content="robots, workshop, IROS, International Conference on Intelligent Robots and Systems">
    <meta name="author" content="Omur Arslan, Nikolay Atanasov, Mehmet Dogar, H. Jin Kim, Rafael Papallas">
	<link rel="shortcut icon" href="favicon.ico"> 
	
	<!-- Google Font -->
	<link href="https://fonts.googleapis.com/css?family=Montserrat:600,700,800|Roboto:300,400,700&display=swap" rel="stylesheet">
	
	<!-- FontAwesome JS-->
	<script defer src="assets/fontawesome/js/all.min.js"></script>

	<!-- Theme CSS -->  
	<link id="theme-style" rel="stylesheet" href="assets/css/theme.css">
</head> 

<body>    
	
	
	<section id="schedule-section" class="schedule-section section">
		<div class="container">
			<h3 class="section-heading text-center">IROS 2023 IPPC Workshop Schedule</h3>
			<h4 class="text-center mb-3">Workshop on Integrated Perception, Planning, and Control for Physically and Contextually-Aware Robot Autonomy</h4>
			<h5 class="text-center mb-3"><i class="fa-solid fa-building"></i> Room: 360 (SuWT13.1)</h5>
            <div class="text-center mb-5">
                <a href="index.html"><button type="button" class="btn btn-primary btn-lg mb-4">Tap here to access full workshop website</button></a>
            </div>
			
			<!-- Tab panes -->
			<div class="schedule-tab-content tab-content">
				<div class="tab-pane active offset-lg-2" id="tab-1-content" role="tabpanel" aria-labelledby="tab-1">
					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">08:50 - 09:00</h4>
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3"><i class="fa-solid fa-flag-checkered"></i> Opening and welcome</h3>
                            <div class="col-xs-12" style="height:25px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">09:00 - 09:30</h4>
							<div class="profile">
								<a href="http://www.kostasalexis.com" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/kostas.png" alt=""></a>
                                <div class="name"><a class="theme-link" href="http://www.kostasalexis.com" target="_blank">Kostas Alexis</a><br>Norwegian University of Science and Technology</div>
							</div><!--//profile-->
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Resilient Autonomy in Perceptually-degraded Environments</h3>
                            <div class="desc"><b>Abstract:</b> Enabling autonomous robots to access, navigate and broadly operate in perceptually-degraded industrial or natural environments represents a strenuous and daunting challenge. Motivated by this fact, this talk focuses on methods and systems toward instilling resilient autonomy - using both classical and data-driven methods - across diverse robot configurations with the aim to seamlessly access and operate anywhere and subject to any conditions. Results and experiences from the victorious journey of Team CERBERUS in the DARPA Subterranean Challenge are presented, lessons learned are outlined and a multitude of experimental studies from follow-up research activities are discussed.</div>
                            <div class="col-xs-12" style="height:20px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">09:30 - 10:00</h4>
							<div class="profile">
								<a href="https://efipsom.github.io" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/efi.png" alt=""></a>
                                <div class="name"><a class="theme-link" target="_blank" href="https://efipsom.github.io">Efi Psomopoulou</a><br>University of Bristol</div>
							</div><!--//profile-->
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Physically Interactive Robots</h3>
                            <div class="desc"><b>Abstract:</b> TBC</div>
                            <div class="col-xs-12" style="height:120px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">10:00 - 10:30</h4>
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Poster Session 1</h3>
                            <button id="toggleButton1" class="btn btn-primary">Hide/Show Papers (downloadable PDFs)</button>
                            <div id="toggleDiv1" class="d-none desc mt-3">
                                <ol>
                                    <li><a target="_blank" href="/papers/gupta.pdf">"LIVE: Lidar Informed Visual Search for Multiple Objects with Multiple Robots" by Ryan Gupta, Minkyu Kim, Juliana Rodriguez, Kyle Morgenstein, and Luis Sentis</a></li>
                                    <li><a target="_blank" href="/papers/petracek.pdf">"Cooperative UAV Autonomy of Dronument: New Era in Cultural Heritage Preservation" by Pavel Petracek, Vit Kratky, Matej Petrlik, and Martin Saska</a></li>
                                    <li><a target="_blank" href="/papers/lin.pdf">"Semantic-SuPer: Employing Semantic Perception for Endoscopic Tissue Identification, Reconstruction, and Tracking" by Shan Lin, Jingpei Lu, Florian Richter, and Michael Yip</a></li>
                                    <li><a target="_blank" href="/papers/zhu.pdf">"Dynamic Object Avoidance using Event-Data for a Quadruped Robot" by Shifan Zhu, Nisal Perera, Shangqun Yu, Hochul Hwang, and Donghyun Kim</a></li>
                                    <li><a target="_blank" href="/papers/cho.pdf">"Enhancing Autonomous Reinforcement Learning: A Demonstration-Free Approach via Implicit and Bidirectional Curriculum" by Daesol Cho, Jigang Kim, Hyoun, and Jin Kim</a></li>
                                    <li><a target="_blank" href="/papers/ewen.pdf">"Multi-Modal Semantic Perception Using Bayesian Inference" by Parker Ewen, Gitesh Gunjal, Hao Chen, Anran Li, Yuzhen Chen, and Ram Vasudevan</a></li>
                                    <li><a target="_blank" href="/papers/nayak.pdf">"Cooperative Probabilistic Trajectory Forecasting under Occlusion" by Anshul Nayak and Azim Eskandarian</a></li>
                                    <li><a target="_blank" href="/papers/fei_liu.pdf">"Shape Reconstruction of Soft, Continuum Robots using Differentiable Rendering with Geometrical Shape Primitive" by Fei Liu and Michael Yip</a></li>
                                    <li><a target="_blank" href="/papers/zhan.pdf">"PyPose v0.6: The Imperative Programming Interface for Robotics" by Zitong Zhan, Xiangfu Li, Qihang Li, Haonan He, Abhinav Pandey, Haitao Xiao, Yangmengfei Xu, Xiangyu Chen, Kuan Xu, Kun Cao, Zhipeng Zhao, Zihan Wang, Huan Xu, Zihang Fang, Yutian Chen, Wentao Wang, Xu Fang, Yi Du, Tianhao Wu, Xiao Lin, Yuheng Qiu, Fan Yang, Jingnan Shi, Shaoshu Su, Yiren Lu, Taimeng Fu, Karthik Dantu, Jiajun Wu, Lihua Xie, Marco Hutter, Luca Carlone, Sebastian Scherer, Daning Huang, Yaoyu Hu, Junyi Geng, Chen Wang</a></li>
                                    <li><a target="_blank" href="/papers/minghan_zhu.pdf">"Monocular 3D Object Detection with Viewpoint-Invariant Inter-Object Estimation for Better Contextual Behavior Understanding" Minghan Zhu</a></li>
                                    <li><a target="_blank" href="/papers/sarmiento.pdf">"Gripper-Aware GraspNet: End-Effector Shape Context for Cross-Gripper Generalization" Alina Sarmiento, Anthony Simeonov, and Pulkit Agrawal</a></li>
                                    <li><a target="_blank" href="/papers/shi.pdf">“Inflatable Fingertips with Stretchable Pressure Sensors for Adaptive Grasping and Manipulation,” by Hongyang Shi and Nanshu Lu</a></li>
                                </ol>
                            </div>
                            <div class="col-xs-12" style="height:10px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">10:30 - 11:00</h4>
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3"><i class="fas fa-coffee"></i> Coffee Break</h3>
                            <div class="col-xs-12" style="height:10px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">11:00 - 11:30</h4>
							<div class="profile">
								<a href="https://robertomartinmartin.com" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/roberto.png" alt=""></a>
                                <div class="name"><a class="theme-link" href="https://robertomartinmartin.com" target="_blank">Roberto Martin-Martin</a><br>University of Texas at Austin</div>
							</div><!--//profile-->
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Perceiving to Interact, Interacting to Perceive</h3>
                            <div class="desc"><b>Abstract:</b> Humans and other biological agents interact with the environment to obtain the information for their tasks. Differently, robots are “afraid” of contacting the environment, strongly relying on passive modes of information gathering, which restrict their capabilities. In this talk, I will present my past and recent work to endorse robots with interactive capabilities to perceive their environment, for example, to find objects of interest and manipulate them. I will also present our recent effort to develop an affordable robotic hand that is friendly with the type of behavior my lab wants to create in robots: full of contact and where learning is enabled by physical interactions.</div>
                            <div class="col-xs-12" style="height:20px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">11:30 - 11:50</h4>
							<div class="profile">
								<a href="https://research.nvidia.com/person/peter-karkus" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/karkus.jpg" alt=""></a>
                                <div class="name"><a class="theme-link" href="https://research.nvidia.com/person/peter-karkus" target="_blank">Peter Karkus</a><br>NVIDIA</div>
							</div><!--//profile-->
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Invited PhD Talk: Differentiable Robotics: Integrated Perception, Planning, and Control with Differentiable Algorithm Networks</h3>
                            <div class="desc"><b>Abstract:</b> What architecture will scale to human-level robot intelligence? Classical perception-planning-control methods often assume perfect models and tracktable optimization; learning-based methods are data hungry and often fail to generalize. In this talk I will introduce the Differentiable Algorithm Network (DAN), a compositional framework that fuses classical algorithmic architectures and deep neural networks. A DAN is composed of neural network modules that each encode a differentiable robot algorithm, and it is trained end-to-end from data. I will illustrate the potentials of the DAN framework through applications including visual robot navigation and autonomous vehicle control.</div>
                            <div class="col-xs-12" style="height:20px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">11:50 - 12:20</h4>
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Panel Discussion: Robot Perception for Navigation and Manipulation</h3>
                            <div class="desc"><b>Abstract:</b> Robots capable of autonomous navigation and manipulation with advanced perception and decision-making skills offer tremendous potential to assist people with challenging and repetitive tasks. Robots sharing the same environment with people need novel semantic planning objectives that integrate perception and planning at a high level to generate contextually relevant robot behavior. This panel discussion explores the cutting-edge field of robot perception and its critical role in enabling robots to navigate and manipulate objects in complex environments. Coline, Efi, Peter, and Roberto will discuss the challenges and emerging opportunities. The session will be chaired by Nikolay Atanasov.</div>
                            <div class="col-xs-12" style="height:20px;"></div>
                            <div class="text-center">
                                <div class="row">
                                    <div class="col-3 col-lg-3 mb-4">
                                        <a href="https://cdevin.github.io" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/coline.jpg" alt=""></a>
                                        <div class="name"><a class="theme-link" href="https://cdevin.github.io" target="_blank">Coline Devin</a><br>Google DeepMind</div>
                                    </div><!--//profile-->

                                    <div class="col-3 col-lg-3 mb-4">
                                        <a href="https://efipsom.github.io" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/efi.png" alt=""></a>
                                        <div class="name"><a class="theme-link" target="_blank" href="https://efipsom.github.io">Efi Psomopoulou</a><br>University of Bristol</div>
                                    </div><!--//profile-->

                                    <div class="col-3 col-lg-3 mb-4">
                                        <a href="https://research.nvidia.com/person/peter-karkus" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/karkus.jpg" alt=""></a>
                                        <div class="name"><a class="theme-link" href="https://research.nvidia.com/person/peter-karkus" target="_blank">Peter Karkus</a><br>NVIDIA</div>
                                    </div><!--//profile-->

                                    <div class="col-3 col-lg-3 mb-4">
                                        <a href="https://robertomartinmartin.com" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/roberto.png" alt=""></a>
                                        <div class="name"><a class="theme-link" href="https://robertomartinmartin.com" target="_blank">Roberto Martin-Martin</a><br>University of Texas at Austin</div>
                                    </div><!--//profile-->
                                </div>
                                <div class="row">
                                    <h4>Chair</h4>
                                    <div class="profile">
                                        <a href="https://rpapallas.com" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/organisers/nikolay.png" alt=""></a>
                                        <div class="name"><a class="theme-link" href="http://rpapallas.com" target="_blank">Nikolay Atanasov</a><br>University of California San Diego</div>
                                    </div><!--//profile-->
                                </div>
                            </div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">12:20 - 13:30</h4>
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3"><i class="fa-solid fa-utensils"></i> Lunch Break</h3>
                            <div class="col-xs-12" style="height:10px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">13:30 - 14:00</h4>
							<div class="profile">
								<a href="https://cse.umn.edu/ece/changhyun-choi" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/changhyun.png" alt=""></a>
                                <div class="name"><a class="theme-link" href="https://cse.umn.edu/ece/changhyun-choi" target="_blank">Changhyun Choi</a><br>University of Minnesota</div>
							</div><!--//profile-->
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Interactive Robotic Object Perception and Manipulation</h3>
                            <div class="desc"><b>Abstract:</b> Robotic manipulation of diverse objects in unstructured environments remains a major challenge in the field of robotics. Humans adeptly search, grasp, and manipulate objects across a variety of challenging scenarios, whereas current state-of-the-art robotic systems fail to achieve human-level proficiency. The overarching goal of my research is to develop computational learning models that enable robots to perceive the world, acquire new skills, and perform dexterous manipulation at or beyond human capabilities. In this workshop, I will present recent progress from my research group toward the research goal. Specifically, I will discuss (1) object searching and grasping under occlusion and clutter and (2) interactive manipulation for object segmentation. Both projects aim to integrate perception and decision-making to address key challenges in robotic perception and manipulation.</div>
                            <div class="col-xs-12" style="height:20px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">14:00 - 14:30</h4>
							<div class="profile">
								<a href="https://cdevin.github.io" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/coline.jpg" alt=""></a>
                                <div class="name"><a class="theme-link" href="https://cdevin.github.io" target="_blank">Coline Devin</a><br>Google DeepMind</div>
							</div><!--//profile-->
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Transformer-based policies for multi-task robotic manipulation</h3>
                            <div class="desc"><b>Abstract:</b> Robot learning has been difficult to scale due to cost of obtaining data for each additional task. This talk will discuss how we can instead benefit from broad, robot-agnostic knowledge about the world and then improve by reducing the cost of acquiring each next task.</div>
                            <div class="col-xs-12" style="height:90px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">14:30 - 15:00</h4>
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Poster Session 2</h3>
                            <button id="toggleButton2" class="btn btn-primary">Hide/Show Papers (downloadable PDFs)</button>
                            <div id="toggleDiv2" class="d-none desc mt-3">
                            <ol>
                                <li><a target="_blank" href="/papers/xing.pdf">"Autonomous Power Line Inspection with Drones via Perception-Aware MPC" by Jiaxu Xing, Giovanni Cioffi, Javier Hidalgo-Carrió, and Davide Scaramuzza</a></li>
                                <li><a target="_blank" href="/papers/wu.pdf">"iPLAN: Intent-Aware Planning in Heterogeneous Traffic via Distributed Multi-Agent Reinforcement Learning" by Xiyang Wu, Rohan Chandra, Tianrui Guan, Amrit Singh Bedi, and Dinesh Manocha</a></li>
                                <li><a target="_blank" href="/papers/petracek_2.pdf">"Degradation-Aware Point Cloud Sampling in Robot Ego-Motion Estimation" by Pavel Petracek, Nikhil Khedekar, Morten Nissov, Kostas Alexis, and Martin Saska</a></li>
                                <li><a target="_blank" href="/papers/dashora.pdf">"Imitative Models for Passenger-Scale Autonomous Off-Road Driving" by Nitish R Dashora (University of California, Berkeley)*; Sunggoo Jung (KAIST); Dhruv Shah, Valentin Ibars, Osher Lerner, Chanyoung Jung, Rohan A Thakker, Nicholas Rhinehart, and  Ali Agha</a></li>
                                <li><a target="_blank" href="/papers/bauer.pdf">"HiFaive: Learning Human-inspired Dexterous Manipulation with the Faive Robotic Hand" by Erik Bauer, Elvis Nava, and Robert Kevin Katzschmann</a></li>
                                <li><a target="_blank" href="/papers/koga.pdf">"Hide and Seek with Visibility Constraints using Control Barrier Functions" by Shumon Koga, Minnan Zhou, Nikolay Atanasov, and Dimitra Panagou</a></li>
                                <li><a target="_blank" href="/papers/suresh.pdf">"Greedy Perspectives: Dynamic Multi-Drone View Planning for Collaborative Coverage" by Krishna Suresh, Aditya Rauniyar, Micah Corah, and Sebastian Scherer</a></li>
                                <li><a target="_blank" href="/papers/liu.pdf">"Risk-Aware Multi-Robot Target Tracking with Dangerous Zones" by Jiazhen Liu, Peihan Li, Yuwei Wu, Vijay Kumar, and Lifeng Zhou</a></li>
                                <li><a target="_blank" href="/papers/qi.pdf">"General In-Hand Object Rotation with Vision and Touch" by Haozhi Qi, Brent H Yi, Sudharshan Suresh, Mike Lambeta, Yi Ma, Roberto Calandra, and Jitendra Malik</a></li>
                                <li><a target="_blank" href="/papers/asselmeier.pdf">"Hierarchical Multi-modal Quadruped Navigation for Experience-informed Rebar Grid Traversal" by Max Asselmeier, Eohan George, Patricio A Vela, and Ye Zhao</a></li>
                                <li><a target="_blank" href="/papers/liang.pdf">"Bridging Real-to-Sim Gaps through Online Stiffness Optimization with Perception-Enabled Residual Mapping" by Xiao Liang, Fei Liu, Yutong Zhang, and Michael Yip</a></li>
                                <li><a target="_blank" href="/papers/karnan.pdf">“Aligning Robot Navigation Behaviors with Human Intentions and Preferences,” by Haresh Karnan</a></li>
                            </ol>
                            </div>
                            <div class="col-xs-12" style="height:10px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">15:00 - 15:30</h4>
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3"><i class="fas fa-coffee"></i> Coffee Break</h3>
                            <div class="col-xs-12" style="height:10px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">15:30 - 16:00</h4>
							<div class="profile">
								<a href="https://knuasl.wixsite.com/asl-lab" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/hyeonbeom.png" alt=""></a>
                                <div class="name"><a class="theme-link" href="https://knuasl.wixsite.com/asl-lab" target="_blank">Hyeonbeom Lee</a><br>Ajou University</div>
							</div><!--//profile-->
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Autonomous Navigation of Outdoor Mobile Robot Using Monocular Depth Estimation</h3>
                            <div class="desc"><b>Abstract:</b> This study presents a viable approach for outdoor mobile robots by integrating perception, planning, and experimentation. For the perception of mobile robots, we develop a real-time depth estimation algorithm that is gaining interest as a viable alternative to large and heavy sensors, such as LiDAR (light detection and ranging) sensors.  To achieve this goal, we first designed a depth estimation network for a wide-FOV stereo camera. Then, we estimated the depth image using a convolutional neural network and improved the accuracy using stereo-matching. By exploiting our proposed planning algorithm with an optimization approach, we conducted experiments using a real drone and ground mobile robot in an outdoor environment to prove the performance. The experimental results are analyzed, and we further discuss precautions for operating outdoor mobile robots.</div>
                            <div class="col-xs-12" style="height:10px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">16:00 - 16:30</h4>
							<div class="profile">
								<a href="https://irosalab.com/people/georgia-chalvatzaki/" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/georgia.png" alt=""></a>
                                <div class="name"><a class="theme-link" href="https://irosalab.com/people/georgia-chalvatzaki/" target="_blank">Georgia Chalvatzaki</a><br>TU Darmstadt</div>
							</div><!--//profile-->
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Interactive Robot Perception and Learning for Mobile Manipulation</h3>
                            <div class="desc"><b>Abstract:</b> TBC</div>
                            <div class="col-xs-12" style="height:120px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">16:30 - 16:50</h4>
                            
                            <div class="profile">
                                <a href="http://rohanchandra30.github.io" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/rohan.jpg" alt=""></a>
                                <div class="name"><a class="theme-link" href="http://rohanchandra30.github.io" target="_blank">Rohan Chandra</a><br>UT Austin</div>
                            </div><!--//profile-->
                            
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Invited PhD Talk: Human-like Mobility to Deploy Robots... Everywhere!</h3>
                            <div class="desc"><b>Abstract:</b> Deploying intelligent mobile robots in the real world has been a longstanding goal in robotics and AI. These environments are often dense, heterogeneous, constrained, and unstructured. In this talk, I'll discuss my research on enabling intelligent mobile robots to navigate such complex environments by instilling human-like mobility in robots. In particular, my talk will describe advanced computer vision and machine learning techniques for improved tracking of dynamic entities in dense traffic. It will also introduce an innovative model for estimating drivers' risk preferences by conceptualizing traffic as an undirected dynamic graph and applying the risk estimation algorithm to resolve conflicts between drivers at unsignalized intersections and during merging. Lastly, the talk will offer insights into the creation of simulators, tools, and datasets to spur further research, providing the audience with a comprehensive understanding of the advancements and innovations in intelligent mobile robot navigation.</div>
                            <div class="col-xs-12" style="height:20px;"></div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">16:50 - 17:20</h4>
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3">Panel Discussion: Perception-Aware Robot Navigation and Manipulation</h3>
                            <div class="desc"><b>Abstract:</b> Recent advances in artificial perception enable robots to have semantic understanding and contextual awareness of their surroundings. Similarly, recent years have seen significant progress in decision-making for autonomous navigation and manipulation in complex situations. However, the gap between robot perception and decision-making remains large, as many techniques continue to rely on separation principles between perception, planning, and control. This panel discussion gathers experts at the forefront of this dynamic domain to explore the latest developments, challenges, and opportunities in perception-driven robot navigation and manipulation. Changhyun, Georgia, and Rohan will be talking about this gap and share their insights about the future direction of perception-aware robot navigation and manipulation. The session will be chaired by Rafael Papallas.</div>
                            <div class="col-xs-12" style="height:20px;"></div>
                            <div class="text-center">
                                <div class="row">
                                    <div class="col-4 col-lg-4 mb-4">
                                        <a href="http://rohanchandra30.github.io" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/rohan.jpg" alt=""></a>
                                        <div class="name"><a class="theme-link" href="http://rohanchandra30.github.io" target="_blank">Rohan Chandra</a><br>UT Austin</div>
                                    </div><!--//profile-->
                                    <div class="col-4 col-lg-4 mb-4">
                                        <a href="https://irosalab.com/people/georgia-chalvatzaki/" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/georgia.png" alt=""></a>
                                        <div class="name"><a class="theme-link" href="https://irosalab.com/people/georgia-chalvatzaki/" target="_blank">Georgia Chalvatzaki</a><br>TU Darmstadt</div>
                                    </div><!--//profile-->
                                    <div class="col-4 col-lg-4 mb-4">
                                        <a href="https://cse.umn.edu/ece/changhyun-choi" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/speakers/changhyun.png" alt=""></a>
                                        <div class="name"><a class="theme-link" href="https://cse.umn.edu/ece/changhyun-choi" target="_blank">Changhyun Choi</a><br>University of Minnesota</div>
                                    </div><!--//profile-->
                                </div>
                                <div class="row">
                                    <h4>Chair</h4>
                                    <div class="profile">
                                        <a href="https://rpapallas.com" target="_blank"><img class="profile-image rounded-circle mb-2" src="assets/images/organisers/rafael.png" alt=""></a>
                                        <div class="name"><a class="theme-link" href="http://rpapallas.com" target="_blank">Rafael Papallas</a><br>University of Leeds</div>
                                    </div><!--//profile-->
                                </div>
                            </div>
						</div><!--//content-->
					</div><!--//item-->

					<div class="item item-talk">
						<div class="meta">
							<h4 class="time mb-3">17:20 - 17:30</h4>
						</div><!--//meta-->
						<div class="content">
							<h3 class="title mb-3"><i class="fa-solid fa-flag-checkered"></i> Closing Remarks</h3>
						</div><!--//content-->
					</div><!--//item-->
				</div><!--//tab-1-content-->
			</div><!--//schedule-tab-content-->
		</div><!--//container-->
	</section><!--//schedule-section-->
	
	<footer class="footer py-5 theme-bg-primary">
		<div class="container text-center">
			
			 <!--/* This template is free as long as you keep the footer attribution link. If you'd like to use the template without the attribution link, you can buy the commercial license via our website: themes.3rdwavemedia.com Thank you for your support. :) */-->
			<small class="copyright">Designed with <i class="fas fa-heart" style="color: #EC645E;"></i> by <a href="http://themes.3rdwavemedia.com" target="_blank">Xiaoying Riley</a> for developers</small>
			
		</div><!--//container-->	    
	</footer>

	<!-- Javascript -->          
	<script src="assets/plugins/popper.min.js"></script>
	<script src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>  
	<script src="assets/plugins/smoothscroll.min.js"></script>
	<script src="assets/plugins/gumshoe/gumshoe.polyfills.min.js"></script> 
	<script src="assets/js/main.js"></script> 

    <script>
        document.addEventListener("DOMContentLoaded", function () {
            const toggleButton = document.getElementById("toggleButton1");
            const toggleDiv = document.getElementById("toggleDiv1");

            toggleButton.addEventListener("click", function () {
                if (toggleDiv.classList.contains("d-none")) {
                    toggleDiv.classList.remove("d-none");
                } else {
                    toggleDiv.classList.add("d-none");
                }
            });

            const toggleButton2 = document.getElementById("toggleButton2");
            const toggleDiv2 = document.getElementById("toggleDiv2");

            toggleButton2.addEventListener("click", function () {
                if (toggleDiv2.classList.contains("d-none")) {
                    toggleDiv2.classList.remove("d-none");
                } else {
                    toggleDiv2.classList.add("d-none");
                }
            });
        });
    </script>
</body>
</html> 

